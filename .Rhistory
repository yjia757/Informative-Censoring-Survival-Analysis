eps_1 = runif(n,-1,1)
eps_2 = runif(n)
eps_3 = runif(n)
L = 0.5*eps_1 + rnorm(n,0, sd = 1)  # mean zero
U = eps_1^2 + rnorm(n,0, sd = 1.5)  # mean 1/3
Z = cbind(L,U)
c = -log(eps_2)/exp(as.vector(Z%*%c(-1, 2)))
t = -log(eps_3)/exp(-1 + as.vector(Z%*%c(1, 1.5)))
}
X = pmin(t,c)
Delta = as.numeric(t < c)
duplicated_index = unique(c(which(duplicated(X)), which(duplicated(t)), which(duplicated(c))))
if (length(duplicated_index) != 0) {
cat(paste(length(duplicated_index) ,'duplicates detected.\n'))
return(cbind(t, c, X, Delta, L, U)[-duplicated_index,][1:n.original,])
} else
return(cbind(t, c, X, Delta, L, U)[1:n.original,])
}
# generate data for debugging purpose, check nuisance estimation
set.seed(2008)
data_debug = simulate(1, 2500)[,-(1:2)]
tau = max(data_debug[,1]) + 1  # set maximum follow-up time
#tau = 1
data_debug[,2] = data_debug[,2]*(data_debug[,1] < tau)
data_debug[,1] = ifelse(data_debug[,1] < tau, data_debug[,1], tau)
# check censoring rate
cen.rate = mean(data_debug[,2])
print(cen.rate)
# function simulating data
simulate = function(scenario = 1, n = 1000){
n.original=n
n = ceiling(n*1.01)
if (scenario == 1){
eps_1 = runif(n,-1,1)
eps_2 = runif(n)
eps_3 = runif(n)
L = 0.5*eps_1 + rnorm(n,0, sd = 1)  # mean zero
U = eps_1^2 + rnorm(n,0, sd = 1.5)  # mean 1/3
Z = cbind(L,U)
c = -log(eps_2)/exp(as.vector(Z%*%c(-1, 2)))
t = -log(eps_3)/exp(as.vector(Z%*%c(1, 1.5)))
}
X = pmin(t,c)
Delta = as.numeric(t < c)
duplicated_index = unique(c(which(duplicated(X)), which(duplicated(t)), which(duplicated(c))))
if (length(duplicated_index) != 0) {
cat(paste(length(duplicated_index) ,'duplicates detected.\n'))
return(cbind(t, c, X, Delta, L, U)[-duplicated_index,][1:n.original,])
} else
return(cbind(t, c, X, Delta, L, U)[1:n.original,])
}
# generate data for debugging purpose, check nuisance estimation
set.seed(2008)
data_debug = simulate(1, 2500)[,-(1:2)]
tau = max(data_debug[,1]) + 1  # set maximum follow-up time
#tau = 1
data_debug[,2] = data_debug[,2]*(data_debug[,1] < tau)
data_debug[,1] = ifelse(data_debug[,1] < tau, data_debug[,1], tau)
# check censoring rate
cen.rate = mean(data_debug[,2])
print(cen.rate)
# this file is for debugging Cox, Spline, and RSF implemented in sim_full_debug.R
# which were originally used in Jiyu's code to generate matrices S_t.
# RSF require memory that my PC is not support. So focus on Cox and Spline.
library(survival)
library(parallel)
library(randomForestSRC)
library(polspline)
library(flexsurv)
library(actuar)
# function simulating data
simulate = function(scenario = 1, n = 5000){
n.original=n
n = ceiling(n*1.01)
if (scenario == 1){
eps_1 = runif(n,-1,1)
eps_2 = runif(n)
eps_3 = runif(n)
L = 0.5*eps_1 + rnorm(n,0, sd = 1)
U = eps_1^2 + rnorm(n,0, sd = 0.3)
Z = cbind(L,U)
t = -log(eps_2)/exp(as.vector(Z%*%c(-1, 2)))
c = -log(eps_3)/exp(as.vector(Z%*%c(1, 1.5)))
}
X = pmin(t,c)
Delta = as.numeric(t < c)
duplicated_index = unique(c(which(duplicated(X)), which(duplicated(t)), which(duplicated(c))))
if (length(duplicated_index) != 0) {
cat(paste(length(duplicated_index) ,'duplicates detected.\n'))
return(cbind(t, c, X, Delta, L, U)[-duplicated_index,][1:n.original,])
} else
return(cbind(t, c, X, Delta, L, U)[1:n.original,])
}
# generate data for debugging purpose, check nuisance estimation
set.seed(2008)
data_debug = simulate(1, 5000)[,-(1:2)] # Use larger sample size since I won't use cross fitting as usual
tau = max(data_debug[,1]) + 1  # set maximum follow-up time
#tau = 1
data_debug[,2] = data_debug[,2]*(data_debug[,1] < tau)
data_debug[,1] = ifelse(data_debug[,1] < tau, data_debug[,1], tau)
# check censoring rate
X = data_debug[,1]
Delta = data_debug[,2]
Delta_c = (1 - Delta)*(X < tau)
cen.rate = mean(Delta_c)
print(cen.rate)
# check overlap assumption
beta.true = c(-1, 2)
#beta.true = c(0, 2)
Lambda.overlap = exp(as.matrix(data_debug[,-(1:2)])%*%beta.true) * tau
surv.overlap = exp(-Lambda.overlap)
overlap = sum(surv.overlap == 0)
print(overlap)
# split training and testing
data_debug_train = data_debug[-10,]  # 4999 x 4 matrix
data_debug_test = data_debug[10,]  # 4 x 1 vector
# true survival function
X.sort = sort(unique(data_debug_train[,1]))
Lambda.true = exp(matrix(data_debug_test[-(1:2)], 1, 2)%*%beta.true) %*% t(X.sort)# baseline hazard is 1
surv.true = exp(-Lambda.true)
# estimated survival function using Cox
model.cox = coxph(Surv(X, Delta) ~ ., timefix = F, data = as.data.frame(data_debug_train))
Lambda.cox = exp(matrix(data_debug_test[-(1:2)], 1, 2)%*%model.cox$coefficients) %*% t(basehaz(model.cox, centered = F)$hazard)
surv.est.cox = exp(-Lambda.cox)
plot(X.sort, surv.true, col = 1, type = "l",lty = 1,
main = "survival curves",
xlab = "time", ylab = "Survival probability")
lines(X.sort, surv.est.cox, col = 2, type = "l", lty = 2, lwd = 2.5)
legend("topright",
legend = c("True Survival", "Estimated Cox"),
col = 1:2, lty = 1:2, lwd = c(1, 2.5), bty = "n")
# true survival function
X.sort.full = sort(unique(data_debug[,1]))
Lambda.true.long = exp(matrix(data_debug_test[-(1:2)], 1, 2)%*%beta.true) %*% t(X.sort.full)# baseline hazard is 1
surv.true.long = exp(-Lambda.true.long)
# estimated survival function using Spline
X = data_debug_train[,1]
Delta = data_debug_train[,2]
L = data_debug_train[,3]
U = data_debug_train[,4]
model.spline = hare(X, Delta, cbind(L,U))
surv.est.spline = 1 - sapply(X.sort.full, function(x) phare(q = x, cov = data_debug_test[-(1:2)], fit = model.spline))
plot(X.sort.full, surv.true.long, col = 1, type = "l",lty = 1,
main = "survival curves",
xlab = "time", ylab = "Survival probability")
lines(X.sort.full, surv.est.spline, col = 3, type = "l", lty = 3, lwd = 2.5)
legend("topright",
legend = c("True Survival", "Estimated Spline"),
col = c(1,3), lty = c(1,3), lwd = c(1, 2.5), bty = "n")
# this file is for debugging Cox, Spline, and RSF implemented in sim_full_debug.R
# which were originally used in Jiyu's code to generate matrices S_t.
# RSF require memory that my PC is not support. So focus on Cox and Spline.
library(survival)
library(parallel)
library(randomForestSRC)
library(polspline)
library(flexsurv)
library(actuar)
# function simulating data
simulate = function(scenario = 1, n = 5000){
n.original=n
n = ceiling(n*1.01)
if (scenario == 1){
eps_1 = runif(n,-1,1)
eps_2 = runif(n)
eps_3 = runif(n)
L = 0.5*eps_1 + rnorm(n,0, sd = 1)
U = eps_1^2 + rnorm(n,0, sd = 0.3)
Z = cbind(L,U)
t = -log(eps_2)/exp(as.vector(Z%*%c(-1, 2)))
c = -log(eps_3)/exp(as.vector(Z%*%c(1, 1.5)))
}
X = pmin(t,c)
Delta = as.numeric(t < c)
duplicated_index = unique(c(which(duplicated(X)), which(duplicated(t)), which(duplicated(c))))
if (length(duplicated_index) != 0) {
cat(paste(length(duplicated_index) ,'duplicates detected.\n'))
return(cbind(t, c, X, Delta, L, U)[-duplicated_index,][1:n.original,])
} else
return(cbind(t, c, X, Delta, L, U)[1:n.original,])
}
# generate data for debugging purpose, check nuisance estimation
set.seed(2008)
data_debug = simulate(1, 5000)[,-c(1:2)] # Use larger sample size since I won't use cross fitting as usual
tau = max(data_debug[,1]) + 1  # set maximum follow-up time
#tau = 1
data_debug[,2] = data_debug[,2]*(data_debug[,1] < tau)
data_debug[,1] = ifelse(data_debug[,1] < tau, data_debug[,1], tau)
# check censoring rate
cen.rate = mean(data_debug[,2])
print(cen.rate)
# check overlap assumption
beta.true = c(-1, 2)
#beta.true = c(0, 2)
Lambda.overlap = exp(as.matrix(data_debug[,-(1:2)])%*%beta.true) * tau
surv.overlap = exp(-Lambda.overlap)
overlap = sum(surv.overlap == 0)
print(overlap)
# split training and testing
data_debug_train = data_debug[-10,]  # 4999 x 4 matrix
data_debug_test = data_debug[10,]  # 4 x 1 vector
# true survival function
X.sort = sort(unique(data_debug_train[,1]))
Lambda.true = exp(matrix(data_debug_test[-(1:2)], 1, 2)%*%beta.true) %*% t(X.sort)# baseline hazard is 1
surv.true = exp(-Lambda.true)
# estimated survival function using Cox
model.cox = coxph(Surv(X, Delta) ~ ., timefix = F, data = as.data.frame(data_debug_train[,-4]))
Lambda.cox = exp(matrix(data_debug_test[3], 1, 1)%*%model.cox$coefficients) %*% t(basehaz(model.cox, centered = F)$hazard)
surv.est.cox = exp(-Lambda.cox)
plot(X.sort, surv.true, col = 1, type = "l",lty = 1,
main = "survival curves",
xlab = "time", ylab = "Survival probability")
lines(X.sort, surv.est.cox, col = 2, type = "l", lty = 2, lwd = 2.5)
legend("topright",
legend = c("True Survival", "Estimated Cox"),
col = 1:2, lty = 1:2, lwd = c(1, 2.5), bty = "n")
# true survival function
X.sort.full = sort(unique(data_debug[,1]))
Lambda.true.long = exp(matrix(data_debug_test[-(1:2)], 1, 2)%*%beta.true) %*% t(X.sort.full)# baseline hazard is 1
surv.true.long = exp(-Lambda.true.long)
# estimated survival function using Spline
X = data_debug_train[,1]
Delta = data_debug_train[,2]
L = data_debug_train[,3]
model.spline = hare(X, Delta, L)
surv.est.spline = 1 - sapply(X.sort.full, function(x) phare(q = x, cov = data_debug_test[3], fit = model.spline))
plot(X.sort.full, surv.true.long, col = 1, type = "l",lty = 1,
main = "survival curves",
xlab = "time", ylab = "Survival probability")
lines(X.sort.full, surv.est.spline, col = 3, type = "l", lty = 3, lwd = 2.5)
legend("topright",
legend = c("True Survival", "Estimated Spline"),
col = c(1,3), lty = c(1,3), lwd = c(1, 2.5), bty = "n")
# split training and testing
data_debug_train = data_debug[-11,]  # 4999 x 4 matrix
data_debug_test = data_debug[11,]  # 4 x 1 vector
# true survival function
X.sort = sort(unique(data_debug_train[,1]))
Lambda.true = exp(matrix(data_debug_test[-(1:2)], 1, 2)%*%beta.true) %*% t(X.sort)# baseline hazard is 1
surv.true = exp(-Lambda.true)
# estimated survival function using Cox
model.cox = coxph(Surv(X, Delta) ~ ., timefix = F, data = as.data.frame(data_debug_train[,-4]))
Lambda.cox = exp(matrix(data_debug_test[3], 1, 1)%*%model.cox$coefficients) %*% t(basehaz(model.cox, centered = F)$hazard)
surv.est.cox = exp(-Lambda.cox)
plot(X.sort, surv.true, col = 1, type = "l",lty = 1,
main = "survival curves",
xlab = "time", ylab = "Survival probability")
lines(X.sort, surv.est.cox, col = 2, type = "l", lty = 2, lwd = 2.5)
legend("topright",
legend = c("True Survival", "Estimated Cox"),
col = 1:2, lty = 1:2, lwd = c(1, 2.5), bty = "n")
# true survival function
X.sort.full = sort(unique(data_debug[,1]))
Lambda.true.long = exp(matrix(data_debug_test[-(1:2)], 1, 2)%*%beta.true) %*% t(X.sort.full)# baseline hazard is 1
surv.true.long = exp(-Lambda.true.long)
# estimated survival function using Spline
X = data_debug_train[,1]
Delta = data_debug_train[,2]
L = data_debug_train[,3]
model.spline = hare(X, Delta, L)
surv.est.spline = 1 - sapply(X.sort.full, function(x) phare(q = x, cov = data_debug_test[3], fit = model.spline))
plot(X.sort.full, surv.true.long, col = 1, type = "l",lty = 1,
main = "survival curves",
xlab = "time", ylab = "Survival probability")
lines(X.sort.full, surv.est.spline, col = 3, type = "l", lty = 3, lwd = 2.5)
legend("topright",
legend = c("True Survival", "Estimated Spline"),
col = c(1,3), lty = c(1,3), lwd = c(1, 2.5), bty = "n")
# split training and testing
data_debug_train = data_debug[-13,]  # 4999 x 4 matrix
data_debug_test = data_debug[13,]  # 4 x 1 vector
# true survival function
X.sort = sort(unique(data_debug_train[,1]))
Lambda.true = exp(matrix(data_debug_test[-(1:2)], 1, 2)%*%beta.true) %*% t(X.sort)# baseline hazard is 1
surv.true = exp(-Lambda.true)
# estimated survival function using Cox
model.cox = coxph(Surv(X, Delta) ~ ., timefix = F, data = as.data.frame(data_debug_train[,-4]))
Lambda.cox = exp(matrix(data_debug_test[3], 1, 1)%*%model.cox$coefficients) %*% t(basehaz(model.cox, centered = F)$hazard)
surv.est.cox = exp(-Lambda.cox)
plot(X.sort, surv.true, col = 1, type = "l",lty = 1,
main = "survival curves",
xlab = "time", ylab = "Survival probability")
lines(X.sort, surv.est.cox, col = 2, type = "l", lty = 2, lwd = 2.5)
legend("topright",
legend = c("True Survival", "Estimated Cox"),
col = 1:2, lty = 1:2, lwd = c(1, 2.5), bty = "n")
# true survival function
X.sort.full = sort(unique(data_debug[,1]))
Lambda.true.long = exp(matrix(data_debug_test[-(1:2)], 1, 2)%*%beta.true) %*% t(X.sort.full)# baseline hazard is 1
surv.true.long = exp(-Lambda.true.long)
# estimated survival function using Spline
X = data_debug_train[,1]
Delta = data_debug_train[,2]
L = data_debug_train[,3]
model.spline = hare(X, Delta, L)
surv.est.spline = 1 - sapply(X.sort.full, function(x) phare(q = x, cov = data_debug_test[3], fit = model.spline))
plot(X.sort.full, surv.true.long, col = 1, type = "l",lty = 1,
main = "survival curves",
xlab = "time", ylab = "Survival probability")
lines(X.sort.full, surv.est.spline, col = 3, type = "l", lty = 3, lwd = 2.5)
legend("topright",
legend = c("True Survival", "Estimated Spline"),
col = c(1,3), lty = c(1,3), lwd = c(1, 2.5), bty = "n")
# split training and testing
data_debug_train = data_debug[-14,]  # 4999 x 4 matrix
data_debug_test = data_debug[14,]  # 4 x 1 vector
# true survival function
X.sort = sort(unique(data_debug_train[,1]))
Lambda.true = exp(matrix(data_debug_test[-(1:2)], 1, 2)%*%beta.true) %*% t(X.sort)# baseline hazard is 1
surv.true = exp(-Lambda.true)
# estimated survival function using Cox
model.cox = coxph(Surv(X, Delta) ~ ., timefix = F, data = as.data.frame(data_debug_train[,-4]))
Lambda.cox = exp(matrix(data_debug_test[3], 1, 1)%*%model.cox$coefficients) %*% t(basehaz(model.cox, centered = F)$hazard)
surv.est.cox = exp(-Lambda.cox)
# plot comparison
plot1_name <- "L_only_cox_est.png"
plot1_path <- file.path(file_path, plot1_name)
plot(X.sort, surv.true, col = 1, type = "l",lty = 1,
main = "survival curves",
xlab = "time", ylab = "Survival probability")
lines(X.sort, surv.est.cox, col = 2, type = "l", lty = 2, lwd = 2.5)
legend("topright",
legend = c("True Survival", "Estimated Cox"),
col = 1:2, lty = 1:2, lwd = c(1, 2.5), bty = "n")
r
r
r
# true survival function
X.sort.full = sort(unique(data_debug[,1]))
Lambda.true.long = exp(matrix(data_debug_test[-(1:2)], 1, 2)%*%beta.true) %*% t(X.sort.full)# baseline hazard is 1
surv.true.long = exp(-Lambda.true.long)
# estimated survival function using Spline
X = data_debug_train[,1]
Delta = data_debug_train[,2]
L = data_debug_train[,3]
model.spline = hare(X, Delta, L)
surv.est.spline = 1 - sapply(X.sort.full, function(x) phare(q = x, cov = data_debug_test[3], fit = model.spline))
plot(X.sort.full, surv.true.long, col = 1, type = "l",lty = 1,
main = "survival curves",
xlab = "time", ylab = "Survival probability")
lines(X.sort.full, surv.est.spline, col = 3, type = "l", lty = 3, lwd = 2.5)
legend("topright",
legend = c("True Survival", "Estimated Spline"),
col = c(1,3), lty = c(1,3), lwd = c(1, 2.5), bty = "n")
# function simulating data
simulate = function(scenario = 1, n = 5000){
n.original=n
n = ceiling(n*1.01)
if (scenario == 1){
eps_1 = runif(n,-1,1)
eps_2 = runif(n)
eps_3 = runif(n)
L = 0.5*eps_1 + rnorm(n,0, sd = 1)
U = eps_1^2 + rnorm(n,0, sd = 0.3)
Z = cbind(L,U)
t = -log(eps_2)/exp(as.vector(Z%*%c(-1, 2)))
c = -log(eps_3)/exp(as.vector(Z%*%c(1, 1.5)))
}
X = pmin(t,c)
Delta = as.numeric(t < c)
duplicated_index = unique(c(which(duplicated(X)), which(duplicated(t)), which(duplicated(c))))
if (length(duplicated_index) != 0) {
cat(paste(length(duplicated_index) ,'duplicates detected.\n'))
return(cbind(t, c, X, Delta, L, U)[-duplicated_index,][1:n.original,])
} else
return(cbind(t, c, X, Delta, L, U)[1:n.original,])
}
# generate data for debugging purpose, check nuisance estimation
set.seed(2008)
data_debug = simulate(1, 5000)[,-c(1:2)] # Use larger sample size since I won't use cross fitting as usual
tau = max(data_debug[,1]) + 1  # set maximum follow-up time
#tau = 1
data_debug[,2] = data_debug[,2]*(data_debug[,1] < tau)
data_debug[,1] = ifelse(data_debug[,1] < tau, data_debug[,1], tau)
# check censoring rate
cen.rate = mean(data_debug[,2])
print(cen.rate)
# check overlap assumption
beta.true = c(-1, 2)
#beta.true = c(0, 2)
Lambda.overlap = exp(as.matrix(data_debug[,-(1:2)])%*%beta.true) * tau
surv.overlap = exp(-Lambda.overlap)
overlap = sum(surv.overlap == 0)
print(overlap)
# split training and testing
data_debug_train = data_debug[-14,]  # 4999 x 4 matrix
# split training and testing
data_debug_train = data_debug[-15,]  # 4999 x 4 matrix
data_debug_test = data_debug[14,]  # 4 x 1 vector
# true survival function
X.sort = sort(unique(data_debug_train[,1]))
Lambda.true = exp(matrix(data_debug_test[-(1:2)], 1, 2)%*%beta.true) %*% t(X.sort)# baseline hazard is 1
surv.true = exp(-Lambda.true)
# estimated survival function using Cox
model.cox = coxph(Surv(X, Delta) ~ ., timefix = F, data = as.data.frame(data_debug_train[,-4]))
Lambda.cox = exp(matrix(data_debug_test[3], 1, 1)%*%model.cox$coefficients) %*% t(basehaz(model.cox, centered = F)$hazard)
surv.est.cox = exp(-Lambda.cox)
plot(X.sort, surv.true, col = 1, type = "l",lty = 1,
main = "survival curves",
xlab = "time", ylab = "Survival probability")
lines(X.sort, surv.est.cox, col = 2, type = "l", lty = 2, lwd = 2.5)
legend("topright",
legend = c("True Survival", "Estimated Cox"),
col = 1:2, lty = 1:2, lwd = c(1, 2.5), bty = "n")
# split training and testing
data_debug_train = data_debug[-16,]  # 4999 x 4 matrix
data_debug_test = data_debug[16,]  # 4 x 1 vector
# true survival function
X.sort = sort(unique(data_debug_train[,1]))
Lambda.true = exp(matrix(data_debug_test[-(1:2)], 1, 2)%*%beta.true) %*% t(X.sort)# baseline hazard is 1
surv.true = exp(-Lambda.true)
# estimated survival function using Cox
model.cox = coxph(Surv(X, Delta) ~ ., timefix = F, data = as.data.frame(data_debug_train[,-4]))
Lambda.cox = exp(matrix(data_debug_test[3], 1, 1)%*%model.cox$coefficients) %*% t(basehaz(model.cox, centered = F)$hazard)
surv.est.cox = exp(-Lambda.cox)
plot(X.sort, surv.true, col = 1, type = "l",lty = 1,
main = "survival curves",
xlab = "time", ylab = "Survival probability")
lines(X.sort, surv.est.cox, col = 2, type = "l", lty = 2, lwd = 2.5)
legend("topright",
legend = c("True Survival", "Estimated Cox"),
col = 1:2, lty = 1:2, lwd = c(1, 2.5), bty = "n")
# split training and testing
data_debug_train = data_debug[-15,]  # 4999 x 4 matrix
data_debug_test = data_debug[15,]  # 4 x 1 vector
# true survival function
X.sort = sort(unique(data_debug_train[,1]))
Lambda.true = exp(matrix(data_debug_test[-(1:2)], 1, 2)%*%beta.true) %*% t(X.sort)# baseline hazard is 1
surv.true = exp(-Lambda.true)
# estimated survival function using Cox
model.cox = coxph(Surv(X, Delta) ~ ., timefix = F, data = as.data.frame(data_debug_train[,-4]))
Lambda.cox = exp(matrix(data_debug_test[3], 1, 1)%*%model.cox$coefficients) %*% t(basehaz(model.cox, centered = F)$hazard)
surv.est.cox = exp(-Lambda.cox)
plot(X.sort, surv.true, col = 1, type = "l",lty = 1,
main = "survival curves",
xlab = "time", ylab = "Survival probability")
lines(X.sort, surv.est.cox, col = 2, type = "l", lty = 2, lwd = 2.5)
legend("topright",
legend = c("True Survival", "Estimated Cox"),
col = 1:2, lty = 1:2, lwd = c(1, 2.5), bty = "n")
# true survival function
X.sort.full = sort(unique(data_debug[,1]))
Lambda.true.long = exp(matrix(data_debug_test[-(1:2)], 1, 2)%*%beta.true) %*% t(X.sort.full)# baseline hazard is 1
surv.true.long = exp(-Lambda.true.long)
# estimated survival function using Spline
X = data_debug_train[,1]
Delta = data_debug_train[,2]
L = data_debug_train[,3]
model.spline = hare(X, Delta, L)
surv.est.spline = 1 - sapply(X.sort.full, function(x) phare(q = x, cov = data_debug_test[3], fit = model.spline))
plot(X.sort.full, surv.true.long, col = 1, type = "l",lty = 1,
main = "survival curves",
xlab = "time", ylab = "Survival probability")
lines(X.sort.full, surv.est.spline, col = 3, type = "l", lty = 3, lwd = 2.5)
legend("topright",
legend = c("True Survival", "Estimated Spline"),
col = c(1,3), lty = c(1,3), lwd = c(1, 2.5), bty = "n")
set.seed(2007)
n = 1000
n.original=n
n = ceiling(n*1.01)
eps_1 = runif(n,-1,1)
eps_2 = runif(n)
eps_3 = runif(n)
L = 0.5*eps_1 + rnorm(n,0, sd = 1)  # mean zero
U = eps_1^2 + rnorm(n,0, sd = 0.3)  # mean 1/3
Z = cbind(L,U)
t = -log(eps_2)/exp(as.vector(Z%*%c(-1, 2)))
c = -log(eps_3)/exp(as.vector(Z%*%c(0.5, 2.5)))
X = pmin(t,c)
Delta = as.numeric(t < c)
data = cbind(t, c, X, Delta, L, U)[1:n.original,]
data = data[,-c(1,2)]
tau = 3 # set maximum follow-up time
data[,2] = data[,2]*(data[,1] < tau)
data[,1] = ifelse(data[,1] < tau, data[,1], tau)
data = as.data.frame(data)
X.full = data[,1]
X.sort.full = sort(unique(X.full))
folds = cut(1:nrow(data), breaks = 2, labels = F)
S_t.temp = S_c.temp = matrix(0, nrow = nrow(data), ncol = length(unique(X.full)))
fold=1
dfs = dfs.c = data[folds==fold,]
dfn = dfn.c = data[folds!=fold,]
X = dfn[,1]
X.sort = sort(unique(X))
Delta = dfn[,2]
Delta_c = (1 - Delta)*(X < tau)
dfn.c$Delta = Delta_c
colnames(dfn.c)[2] = 'Delta_c'
colnames(dfs.c)[2] = 'Delta_c'
L = dfn[,3]
U = dfn[,4]
model = coxph(Surv(X, Delta) ~ ., timefix = F, data = dfn)
print(model$coefficients)
model = coxph(Surv(X, Delta) ~ ., timefix = F, data = dfn)
library(survival)
library(parallel)
library(randomForestSRC)
library(polspline)
library(flexsurv)
library(actuar)
model = coxph(Surv(X, Delta) ~ ., timefix = F, data = dfn)
print(model$coefficients)
model_c = coxph(Surv(X, Delta_c) ~ ., timefix = F, data = dfn.c)
print(model_c$coefficients)
# predict using formula
basehaz = basehaz(model, centered = F)
basehaz
dim(basehaz)
basehaz(model, centered = F)$hazard
dim(basehaz(model, centered = F)$hazard)
